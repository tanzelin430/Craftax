#!/usr/bin/env python3
"""
Craftax Environment Interaction Script
ä½¿ç”¨ Qwen3-14B æ¨¡å‹ä¸ Craftax ç¯å¢ƒè¿›è¡Œäº¤äº’
"""

import os
import jax
import jax.numpy as jnp
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import time
from typing import Dict, List, Any

from llm_agent_wrapper import CraftaxLLMWrapper


class CraftaxQwenAgent:
    """
    ä½¿ç”¨ Qwen3-14B æ¨¡å‹çš„ Craftax æ™ºèƒ½ä½“
    """

    def __init__(
        self, model_path: str, device: str = "cuda", num_parallel_envs: int = 1
    ):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“ - æ¨¡æ‹Ÿcraftax_agent_loop.pyçš„ç¯å¢ƒç®¡ç†æ–¹å¼

        Args:
            model_path: Qwen3-14B æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹
            num_parallel_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆæ¨¡æ‹Ÿcraftax_agent_loop.pyï¼‰
        """
        init_start_time = time.time()

        self.device = device
        self.model_path = model_path
        self.num_parallel_envs = num_parallel_envs

        # 1. ç¯å¢ƒåˆ›å»ºæ—¶é—´
        print(f"ğŸš€ [ENV CREATE START] Creating {num_parallel_envs} environments...")
        env_create_start = time.time()

        self.env_wrapper = CraftaxLLMWrapper(
            env_name="Craftax-Symbolic-v1", max_episode_steps=500
        )

        self._environments = {}  # æ¨¡æ‹Ÿç±»çº§åˆ«ç¯å¢ƒå­˜å‚¨
        for i in range(num_parallel_envs):
            env_id = f"env_{i}"
            self._create_environment(env_id)

        env_create_time = time.time() - env_create_start
        print(
            f"âœ… [ENV CREATE END] {num_parallel_envs} environments created in {env_create_time:.3f}s"
        )

        # åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, padding_side="left"
        )

        # æ·»åŠ pad_tokenå¦‚æœä¸å­˜åœ¨
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto",
        )

        self.model.eval()
        print(f"Model loaded successfully!")

    def _create_environment(self, env_id: str):
        """æ¨¡æ‹Ÿcraftax_agent_loop.py: åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ–°ç¯å¢ƒ"""
        # åˆ›å»ºç¯å¢ƒçŠ¶æ€å­˜å‚¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
        seed = np.random.randint(0, 100000)
        wrapped_obs, state = self.env_wrapper.reset(seed=seed)

        self._environments[env_id] = {
            "current_state": state,
            "current_obs": wrapped_obs,
            "episode_step_count": 0,
            "episode_id": f"ep_{env_id}_{seed}",
            "seed": seed,
        }

    def _get_environment(self, env_id: str):
        """æ¨¡æ‹Ÿcraftax_agent_loop.py: è·å–æŒ‡å®šçš„é¢„åˆ›å»ºç¯å¢ƒ"""
        if env_id not in self._environments:
            self._create_environment(env_id)
        return self._environments[env_id]

    def generate_response(self, prompt: str, max_new_tokens: int = 200) -> str:
        """
        ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå“åº”
        """
        # 2. æ¯ä¸€è½®rolloutæ¨ç†æ—¶é—´
        rollout_start = time.time()

        prompt_list = [{"role": "assistant", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            prompt_list,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False,
        )

        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        response = self.tokenizer.decode(
            outputs[0][model_inputs["input_ids"].shape[1] :], skip_special_tokens=True
        )

        rollout_time = time.time() - rollout_start
        print(f"ğŸ§  [ROLLOUT] Inference time: {rollout_time:.3f}s")

        return response.strip()

    def run_episode(self, max_steps: int = 100, verbose: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode

        Args:
            max_steps: æœ€å¤§æ­¥æ•°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            episodeç»“æœç»Ÿè®¡
        """
        # é‡ç½®ç¯å¢ƒ
        wrapped_obs, state = self.env_wrapper.reset(seed=np.random.randint(0, 10000))

        total_reward = 0.0
        step_count = 0
        done = False

        episode_history = []

        if verbose:
            print("=" * 80)
            print("ğŸ® Starting New Craftax Episode")
            print("=" * 80)

        while not done and step_count < max_steps:
            if verbose:
                print(f"\nğŸ“ Step {step_count + 1}")
                print("-" * 40)

            # è·å–LLMå“åº”
            llm_response = self.generate_response(wrapped_obs)

            if verbose:
                print(f"ğŸ¤– LLM Response: {llm_response}")

            # 3. æ¯ä¸€è½®batch stepçš„äº¤äº’æ—¶é—´
            step_start = time.time()

            # è§£æåŠ¨ä½œ
            action_id = self.env_wrapper.parse_llm_response(llm_response)
            action_name = self.env_wrapper.action_names[action_id]

            # æ‰§è¡ŒåŠ¨ä½œ
            wrapped_obs, state, reward, done, info = self.env_wrapper.step(
                state, action_id
            )
            wrapped_obs = self.env_wrapper.wrap_observation(wrapped_obs)

            step_time = time.time() - step_start
            print(f"ğŸ”„ [STEP] Environment interaction time: {step_time:.3f}s")

            total_reward += reward
            step_count += 1

            if verbose:
                print(f"âš¡ Action: {action_name} (ID: {action_id})")

            # è®°å½•æ­¥éª¤ä¿¡æ¯
            step_info = {
                "step": step_count,
                "action_id": action_id,
                "action_name": action_name,
                "reward": float(reward),
                "done": done,
                "llm_response": llm_response,
            }
            episode_history.append(step_info)

            if verbose:
                print(f"ğŸ’° Reward: {reward:.2f}")
                print(f"ğŸ“Š Total Reward: {total_reward:.2f}")
                if done:
                    print("âœ… Episode Complete!")

        # è¿”å›episodeç»Ÿè®¡
        episode_stats = {
            "total_reward": float(total_reward),
            "total_steps": step_count,
            "completed": done,
            "average_reward": float(total_reward / step_count)
            if step_count > 0
            else 0.0,
            "history": episode_history,
        }

        if verbose:
            print(f"\nğŸ“ˆ Episode Summary:")
            print(f"   Total Steps: {step_count}")
            print(f"   Total Reward: {total_reward:.2f}")
            print(f"   Average Reward: {episode_stats['average_reward']:.2f}")
            print(f"   Completed: {done}")

        return episode_stats

    def run_serial_batch_like_verl_worker(
        self, batch_size: int = 8, steps_per_batch: int = 5
    ) -> Dict[str, Any]:
        """
        æ¨¡æ‹ŸVerlå•ä¸ªworkerå†…çš„ä¸²è¡Œæ‰¹å¤„ç†ï¼š
        - ç®¡ç†batch_sizeä¸ªç¯å¢ƒ
        - æ¯ä¸ªç¯å¢ƒä¸²è¡Œæ‰§è¡Œsteps_per_batchæ­¥
        - æ¨¡æ‹ŸçœŸå®çš„workerå†…æ‰§è¡Œæ¨¡å¼
        """
        print(
            f"ğŸ­ [WORKER SIMULATION] Processing {batch_size} environments serially (like single Verl worker)"
        )

        total_rollout_time = 0
        total_step_time = 0
        batch_results = []

        # ä¸²è¡Œå¤„ç†æ¯ä¸ªç¯å¢ƒ
        for env_idx in range(batch_size):
            env_id = f"env_{env_idx}"
            env_data = self._get_environment(env_id)

            print(
                f"\nğŸ® Processing Environment {env_idx + 1}/{batch_size} (env_id: {env_id})"
            )

            # å¦‚æœç¯å¢ƒéœ€è¦é‡ç½®
            if env_data["current_obs"] is None:
                seed = np.random.randint(0, 100000)
                wrapped_obs, state = self.env_wrapper.reset(seed=seed)
                env_data["current_state"] = state
                env_data["current_obs"] = wrapped_obs
                env_data["episode_step_count"] = 0
                print(f"ğŸ”„ Reset environment {env_id} with seed {seed}")

            episode_reward = 0
            step_count = 0

            # ä¸²è¡Œæ‰§è¡Œå¤šæ­¥
            for step in range(steps_per_batch):
                print(f"ğŸ“ Step {step + 1}/{steps_per_batch}")

                # æ¨¡æ‹Ÿagent_loopçš„å•æ­¥å¤„ç†
                wrapped_obs = env_data["current_obs"]

                # LLMæ¨ç†
                llm_response = self.generate_response(wrapped_obs)
                total_rollout_time += (
                    time.time() - time.time()
                )  # è¿™é‡Œrolloutæ—¶é—´å·²åœ¨generate_responseä¸­ç»Ÿè®¡

                # ç¯å¢ƒäº¤äº’
                step_start = time.time()
                try:
                    action_id = self.env_wrapper.parse_llm_response(llm_response)
                except:
                    action_id = 0

                new_obs, new_state, reward, done, _ = self.env_wrapper.step(
                    env_data["current_state"], action_id
                )
                wrapped_obs = self.env_wrapper.wrap_observation(new_obs)

                step_time = time.time() - step_start
                total_step_time += step_time
                print(f"ğŸ”„ [STEP] Environment interaction time: {step_time:.3f}s")

                # æ›´æ–°ç¯å¢ƒçŠ¶æ€
                env_data["current_state"] = new_state
                env_data["current_obs"] = wrapped_obs
                env_data["episode_step_count"] += 1
                episode_reward += reward
                step_count += 1

                print(f"ğŸ’° Reward: {reward:.2f}, Action: {action_id}")

                if done:
                    print(f"âœ… Episode finished!")
                    env_data["current_obs"] = None  # æ ‡è®°éœ€è¦é‡ç½®
                    break

            batch_results.append(
                {
                    "env_id": env_id,
                    "episode_reward": episode_reward,
                    "steps_completed": step_count,
                }
            )

        # ç»Ÿè®¡ç»“æœ
        total_reward = sum(r["episode_reward"] for r in batch_results)
        total_steps = sum(r["steps_completed"] for r in batch_results)

        print(f"\nğŸ“Š [BATCH SUMMARY]")
        print(f"Total environments processed: {batch_size}")
        print(f"Total steps executed: {total_steps}")
        print(f"Total reward: {total_reward:.2f}")
        print(f"Average reward per env: {total_reward/batch_size:.2f}")
        print(f"Average steps per env: {total_steps/batch_size:.1f}")

        return {
            "batch_size": batch_size,
            "total_steps": total_steps,
            "total_reward": total_reward,
            "results": batch_results,
            "total_step_time": total_step_time,
        }

    def run_multiple_episodes(
        self, num_episodes: int = 5, max_steps_per_episode: int = 100
    ) -> Dict[str, Any]:
        """å‘åå…¼å®¹çš„æ¥å£"""
        return self.run_serial_batch_like_verl_worker(
            batch_size=num_episodes, steps_per_batch=max_steps_per_episode
        )


def main():
    """ä¸»å‡½æ•°"""
    # æ¨¡å‹è·¯å¾„
    model_path = "/fs-computility/mabasic/shared/models/Qwen3-4B"

    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ Model path not found: {model_path}")
        print("Please check the model path and try again.")
        return

    try:
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = CraftaxQwenAgent(model_path=model_path, num_parallel_envs=8)  # é¢„åˆ›å»º8ä¸ªç¯å¢ƒ

        # è¿è¡Œæµ‹è¯•
        results = agent.run_multiple_episodes(num_episodes=8, max_steps_per_episode=5)

        # ä¿å­˜ç»“æœ
        import json

        # è½¬æ¢JAXæ•°ç»„ä¸ºPythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_jax_arrays(obj):
            """é€’å½’è½¬æ¢JAXæ•°ç»„ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, dict):
                return {key: convert_jax_arrays(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_jax_arrays(item) for item in obj]
            elif hasattr(obj, "__array__"):  # JAXæ•°ç»„æˆ–numpyæ•°ç»„
                try:
                    return obj.tolist() if hasattr(obj, "tolist") else float(obj)
                except:
                    return str(obj)
            elif isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj

        # è½¬æ¢ç»“æœ
        serializable_results = convert_jax_arrays(results)

        with open("craftax_experiment_results_4B_LT.json", "w") as f:
            json.dump(serializable_results, f, indent=2)

        print(f"\nâœ… Results saved to craftax_experiment_results.json")

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
