#!/usr/bin/env python3
"""
Craftax Environment Interaction Script
ä½¿ç”¨ Qwen3-14B æ¨¡å‹ä¸ Craftax ç¯å¢ƒè¿›è¡Œäº¤äº’
"""

import os
import jax
import jax.numpy as jnp
import numpy as np
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from typing import Dict, List, Any

from llm_agent_wrapper import CraftaxLLMWrapper


class CraftaxQwenAgent:
    """
    ä½¿ç”¨ Qwen3-14B æ¨¡å‹çš„ Craftax æ™ºèƒ½ä½“
    """
    
    def __init__(self, model_path: str, device: str = "cuda"):
        """
        åˆå§‹åŒ–æ™ºèƒ½ä½“
        
        Args:
            model_path: Qwen3-14B æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ç±»å‹
        """
        self.device = device
        self.model_path = model_path
        
        # åˆå§‹åŒ– Craftax ç¯å¢ƒåŒ…è£…å™¨
        self.env_wrapper = CraftaxLLMWrapper(
            env_name='Craftax-Symbolic-v1',
            max_episode_steps=500
        )
        
        # åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
        print(f"Loading model from {model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="left"
        )
        
        # æ·»åŠ pad_tokenå¦‚æœä¸å­˜åœ¨
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        self.model.eval()
        
        print("Model and environment initialized successfully!")
    
    def generate_response(self, prompt: str, max_new_tokens: int = 200) -> str:
        """
        ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå“åº”
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_new_tokens: æœ€å¤§æ–°tokenæ•°
            
        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        prompt_list = [{"role": "assistant", "content": prompt}]
        text = self.tokenizer.apply_chat_template(
            prompt_list,
            tokenize=False,
            add_generation_prompt=True, 
            enable_thinking=False # Switch between thinking and non-thinking modes. Default is True. We need it to be False for training efficiency.
        )
        # print(f"text: {text}")
        # 2. å°†æ ¼å¼åŒ–åçš„ text è¿›è¡Œ tokenize
        #    æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ ¼å¼åŒ–å¥½çš„ text
        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.device)

        # 3. ç”Ÿæˆå“åº”
        with torch.no_grad():
            outputs = self.model.generate(
                # 4. ç¡®ä¿ä¼ é€’çš„æ˜¯æ ¼å¼åŒ–åçš„ model_inputs
                **model_inputs, 
                max_new_tokens=max_new_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç å“åº”ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        response = self.tokenizer.decode(
            outputs[0][model_inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def run_episode(self, max_steps: int = 100, verbose: bool = True) -> Dict[str, Any]:
        """
        è¿è¡Œä¸€ä¸ªå®Œæ•´çš„episode
        
        Args:
            max_steps: æœ€å¤§æ­¥æ•°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            episodeç»“æœç»Ÿè®¡
        """
        # é‡ç½®ç¯å¢ƒ
        wrapped_obs, state = self.env_wrapper.reset(seed=np.random.randint(0, 10000))
        
        total_reward = 0.0
        step_count = 0
        done = False
        
        episode_history = []
        
        if verbose:
            print("=" * 80)
            print("ğŸ® Starting New Craftax Episode")
            print("=" * 80)
        
        while not done and step_count < max_steps:
            if verbose:
                print(f"\nğŸ“ Step {step_count + 1}")
                print("-" * 40)
            
            # è·å–LLMå“åº” no limit on max new token
            llm_response = self.generate_response(wrapped_obs)
            
            if verbose:
                print(f"ğŸ¤– LLM Response: {llm_response}")
            
            # è§£æåŠ¨ä½œ
            action_id = self.env_wrapper.parse_llm_response(llm_response)
            action_name = self.env_wrapper.action_names[action_id]
            
            if verbose:
                print(f"âš¡ Action: {action_name} (ID: {action_id})")
            
            # æ‰§è¡ŒåŠ¨ä½œ
            wrapped_obs, state, reward, done, info = self.env_wrapper.step(state, action_id)
            wrapped_obs = self.env_wrapper.wrap_observation(wrapped_obs)
            total_reward += reward
            step_count += 1
            
            # è®°å½•æ­¥éª¤ä¿¡æ¯
            step_info = {
                "step": step_count,
                "action_id": action_id,
                "action_name": action_name,
                "reward": float(reward),
                "done": done,
                "llm_response": llm_response
            }
            episode_history.append(step_info)
            
            if verbose:
                print(f"ğŸ’° Reward: {reward:.2f}")
                print(f"ğŸ“Š Total Reward: {total_reward:.2f}")
                if done:
                    print("âœ… Episode Complete!")
        
        # è¿”å›episodeç»Ÿè®¡
        episode_stats = {
            "total_reward": float(total_reward),
            "total_steps": step_count,
            "completed": done,
            "average_reward": float(total_reward / step_count) if step_count > 0 else 0.0,
            "history": episode_history
        }
        
        if verbose:
            print(f"\nğŸ“ˆ Episode Summary:")
            print(f"   Total Steps: {step_count}")
            print(f"   Total Reward: {total_reward:.2f}")
            print(f"   Average Reward: {episode_stats['average_reward']:.2f}")
            print(f"   Completed: {done}")
        
        return episode_stats
    
    def run_multiple_episodes(self, num_episodes: int = 5, max_steps_per_episode: int = 100) -> Dict[str, Any]:
        """
        è¿è¡Œå¤šä¸ªepisodes
        
        Args:
            num_episodes: episodeæ•°é‡
            max_steps_per_episode: æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°
            
        Returns:
            å¤šepisodeç»Ÿè®¡ç»“æœ
        """
        all_episodes = []
        total_rewards = []
        
        print(f"ğŸš€ Running {num_episodes} episodes...")
        
        for episode_idx in range(num_episodes):
            print(f"\n{'='*60}")
            print(f"Episode {episode_idx + 1}/{num_episodes}")
            print('='*60)
            
            episode_stats = self.run_episode(
                max_steps=max_steps_per_episode,
                verbose=True
            )
            
            all_episodes.append(episode_stats)
            total_rewards.append(episode_stats['total_reward'])
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        overall_stats = {
            "num_episodes": num_episodes,
            "episodes": all_episodes,
            "mean_reward": float(np.mean(total_rewards)),
            "std_reward": float(np.std(total_rewards)),
            "max_reward": float(np.max(total_rewards)),
            "min_reward": float(np.min(total_rewards)),
            "completion_rate": float(sum(1 for ep in all_episodes if ep['completed']) / num_episodes)
        }
        
        print(f"\n{'='*60}")
        print("ğŸ“Š OVERALL STATISTICS")
        print('='*60)
        print(f"Episodes: {num_episodes}")
        print(f"Mean Reward: {overall_stats['mean_reward']:.2f} Â± {overall_stats['std_reward']:.2f}")
        print(f"Max Reward: {overall_stats['max_reward']:.2f}")
        print(f"Min Reward: {overall_stats['min_reward']:.2f}")
        print(f"Completion Rate: {overall_stats['completion_rate']:.1%}")
        
        return overall_stats


def main():
    """ä¸»å‡½æ•°"""
    # æ¨¡å‹è·¯å¾„
    model_path = "/fs-computility/mabasic/shared/models/Qwen3-4B"
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ Model path not found: {model_path}")
        print("Please check the model path and try again.")
        return
    
    try:
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = CraftaxQwenAgent(model_path=model_path)
        
        # è¿è¡Œå¤šä¸ªepisodes
        results = agent.run_multiple_episodes(
            num_episodes=1,
            max_steps_per_episode=5
        )
        
        # ä¿å­˜ç»“æœ
        import json
        
        # è½¬æ¢JAXæ•°ç»„ä¸ºPythonåŸç”Ÿç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        def convert_jax_arrays(obj):
            """é€’å½’è½¬æ¢JAXæ•°ç»„ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, dict):
                return {key: convert_jax_arrays(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_jax_arrays(item) for item in obj]
            elif hasattr(obj, '__array__'):  # JAXæ•°ç»„æˆ–numpyæ•°ç»„
                try:
                    return obj.tolist() if hasattr(obj, 'tolist') else float(obj)
                except:
                    return str(obj)
            elif isinstance(obj, (np.integer, np.floating)):
                return obj.item()
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj
        
        # è½¬æ¢ç»“æœ
        serializable_results = convert_jax_arrays(results)
        
        with open("craftax_experiment_results_4B_LT.json", "w") as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"\nâœ… Results saved to craftax_experiment_results.json")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()