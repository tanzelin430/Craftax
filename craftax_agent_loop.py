#!/usr/bin/env python3
"""
Craftax Agent Loop for Verl
ç®¡ç†å®Œæ•´çš„ Craftax ç¯å¢ƒäº¤äº’è¿‡ç¨‹
"""

import asyncio
import time
import numpy as np
from typing import Any, Dict, List
from uuid import uuid4

from transformers import AutoTokenizer
from omegaconf import DictConfig

from verl.experimental.agent_loop.agent_loop import (
    AgentLoopBase,
    AgentLoopOutput,
    AgentLoopMetrics,
)
from llm_agent_wrapper import CraftaxLLMWrapper


class CraftaxAgentLoop(AgentLoopBase):
    """
    Craftax Agent Loop
    ä½¿ç”¨ç±»çº§åˆ«ç¯å¢ƒæŒä¹…åŒ–ï¼Œå› ä¸º Verl æ¡†æ¶ä¼šä¸ºæ¯æ¬¡è¯·æ±‚åˆ›å»ºæ–°çš„å®ä¾‹

    è®¾è®¡æ€è·¯ï¼š
    - Verl ä¸ºæ¯æ¬¡è¯·æ±‚åˆ›å»ºæ–°çš„ Agent Loop å®ä¾‹
    - æˆ‘ä»¬åœ¨ç±»çº§åˆ«å­˜å‚¨ç¯å¢ƒçŠ¶æ€ï¼Œå®ç°è·¨å®ä¾‹çš„ç¯å¢ƒæŒä¹…åŒ–
    - æ¯ä¸ª worker/session å¯¹åº”ä¸€ä¸ªæŒä¹…çš„ç¯å¢ƒ
    """

    _max_episode_steps = None  # ç±»çº§åˆ«é…ç½®ï¼Œä»configåŠ¨æ€è·å–
    _environments = (
        {}
    )  # ç±»çº§åˆ«ç¯å¢ƒå­˜å‚¨: worker_id -> {env_wrapper, current_state, current_obs, episode_step_count, episode_id}
    _worker_counter = 0  # åŸå­è®¡æ•°å™¨
    _worker_lock = None  # çº¿ç¨‹é”
    _assigned_worker_id = None  # å½“å‰workeråˆ†é…åˆ°çš„ID
    _envs_per_worker = 4  # æ¯ä¸ªworkerç®¡ç†çš„ç¯å¢ƒæ•°é‡ï¼Œé»˜è®¤å€¼

    # Wandb reward tracking - ç±»çº§åˆ«å…±äº«
    _episode_cumulative_rewards = {}  # episode_id -> cumulative_reward
    _max_craftax_reward = 226.0  # Craftaxæœ€å¤§å¥–åŠ±å€¼
    _num_envs = 0  # ä»configè·å–å¹¶ç¼“å­˜
    _rollout_n = 0  # ä»configè·å–å¹¶ç¼“å­˜

    def __init__(self, trainer_config, server_manager, tokenizer: AutoTokenizer):
        # ç›´æ¥è°ƒç”¨çˆ¶ç±»çš„__init__ï¼Œçˆ¶ç±»ä¼šå¤„ç†trainer_config.configçš„è®¿é—®
        super().__init__(trainer_config, server_manager, tokenizer)
        self.trainer_config = trainer_config  # å­˜å‚¨traineré…ç½®

        # ä»é…ç½®ä¸­è·å–åºåˆ—é•¿åº¦é™åˆ¶
        self.prompt_length = self.config.actor_rollout_ref.rollout.prompt_length
        self.response_length = self.config.actor_rollout_ref.rollout.response_length

        # ç¯å¢ƒç°åœ¨é€šè¿‡ messages ä¸­çš„ episode_id æ¥æ ‡è¯†ï¼Œä¸éœ€è¦å®ä¾‹çº§åˆ«çš„æ ‡è¯†
        self.current_global_steps = 0  # å­˜å‚¨å½“å‰rolloutçš„global_steps

    @classmethod
    def init_class(cls, config: DictConfig, tokenizer: AutoTokenizer):
        """åˆå§‹åŒ–ç±»å…±äº«é…ç½®å¹¶é¢„åˆ›å»ºå½“å‰workerè´Ÿè´£çš„ç¯å¢ƒ"""
        if cls._class_initialized:
            return

        cls._class_initialized = True
        # ä»é…ç½®ä¸­è¯»å–æœ€å¤§æ­¥æ•°ï¼Œé»˜è®¤ 100
        cls._max_episode_steps = getattr(config, "max_episode_steps", 100)

        # è·å–é…ç½®ä¿¡æ¯å¹¶ç¼“å­˜ä¸ºç±»å˜é‡
        num_episodes = getattr(config.data, "num_episodes", 32)  # æ€»ç¯å¢ƒæ•°
        num_workers = getattr(
            config.actor_rollout_ref.rollout.agent, "num_workers", 32
        )  # workeræ•°é‡

        # ç¼“å­˜Wandbç›¸å…³çš„å›ºå®šé…ç½®
        cls._num_envs = num_episodes
        cls._rollout_n = getattr(config.actor_rollout_ref.rollout, "n")
        # initialize info
        print(
            f"Agent Initialized with num_episodes: {num_episodes}, num_workers: {num_workers}, rollout_n: {cls._rollout_n}"
        )
        # è·å–å½“å‰è¿›ç¨‹ä¿¡æ¯
        import os

        worker_pid = os.getpid()

        # ç®€åŒ–æ–¹æ¡ˆï¼šæ¯ä¸ªAgentLoopå®ä¾‹ç®¡ç†å›ºå®šæ•°é‡çš„ç¯å¢ƒ
        # å› ä¸ºåœ¨ä¸åŒè¿›ç¨‹ä¸­ï¼Œç¯å¢ƒIDå¯ä»¥é‡å¤ï¼Œç”±æ¡†æ¶è´Ÿè´£è·¯ç”±
        cls._envs_per_worker = num_episodes // num_workers  # åŠ¨æ€è®¡ç®—ï¼š128 // 64 = 2

        # å¼ºåˆ¶è¾“å‡ºï¼Œé¿å…è¢«Rayèšåˆ
        import sys

        sys.stdout.flush()
        sys.stderr.flush()

        print(f"ğŸš€ğŸš€ğŸš€ CRAFTAX_WORKER_INIT_PID_{worker_pid} ğŸš€ğŸš€ğŸš€", flush=True)
        print(f"Creating {cls._envs_per_worker} environments", flush=True)

        # æ¯ä¸ªå®ä¾‹åˆ›å»ºå›ºå®šçš„ç¯å¢ƒï¼šenv_0, env_1, ...
        for i in range(cls._envs_per_worker):
            env_id = f"env_{i}"
            cls._create_environment(env_id)

        print(
            f"âœ… AgentLoop (PID {worker_pid}): Initialized {cls._envs_per_worker} environments, max_episode_steps={cls._max_episode_steps}"
        )

    def _get_current_global_step(self) -> int:
        """è·å–å½“å‰rolloutçš„global_steps"""
        print(
            f"ğŸ” Current global_steps: {self.current_global_steps}, rollout_n: {self.__class__._rollout_n}"
        )
        return self.current_global_steps

    @classmethod
    def _create_environment(cls, env_id: str):
        """åˆ›å»ºå¹¶åˆå§‹åŒ–ä¸€ä¸ªæ–°ç¯å¢ƒ"""
        # åˆ›å»ºç¯å¢ƒ
        env_wrapper = CraftaxLLMWrapper(
            env_name="Craftax-Symbolic-v1", max_episode_steps=cls._max_episode_steps
        )

        # åˆå§‹åŒ–ç¯å¢ƒçŠ¶æ€
        seed = np.random.randint(0, 100000)
        current_obs, current_state = env_wrapper.reset(seed=seed)

        cls._environments[env_id] = {
            "env_wrapper": env_wrapper,
            "current_state": current_state,
            "current_obs": current_obs,
            "episode_step_count": 0,
            "episode_id": f"ep_{env_id}_{seed}",
        }

        print(f"ğŸ® Created environment {env_id} with seed {seed}")

    @classmethod
    def _get_environment(cls, env_id: str):
        """è·å–æŒ‡å®šçš„é¢„åˆ›å»ºç¯å¢ƒ"""
        if env_id not in cls._environments:
            raise ValueError(
                f"âŒ Environment {env_id} not found in worker's assigned environments. "
                f"Available environments: {list(cls._environments.keys())}"
            )

        return cls._environments[env_id]

    @classmethod
    def _reset_environment(cls, env_id: str):
        """é‡ç½®æŒ‡å®šç¯å¢ƒåˆ°æ–° episode"""
        env_data = cls._get_environment(env_id)

        # é‡ç½®ç¯å¢ƒ
        seed = np.random.randint(0, 100000)
        current_obs, current_state = env_data["env_wrapper"].reset(seed=seed)

        # æ›´æ–°çŠ¶æ€
        env_data["current_state"] = current_state
        env_data["current_obs"] = current_obs
        env_data["episode_step_count"] = 0
        env_data["episode_id"] = f"ep_{env_id}_{seed}"

        print(f"ğŸ”„ Reset environment {env_id} with seed {seed}")

    async def run(
        self,
        messages: List[Dict[str, Any]],
        sampling_params: Dict[str, Any],
        trajectory: Dict[str, Any] = None,
    ) -> AgentLoopOutput:
        """
        è¿è¡Œå•æ­¥ Craftax ç¯å¢ƒäº¤äº’

        å·¥ä½œæµç¨‹ï¼š
        1. ä» messages ä¸­æå–ç¯å¢ƒæ ‡è¯†ç¬¦
        2. è·å–å¯¹åº”çš„æŒä¹…åŒ–ç¯å¢ƒ
        3. ä½¿ç”¨å½“å‰ç¯å¢ƒè§‚å¯Ÿç”Ÿæˆ LLM å“åº”
        4. è§£æåŠ¨ä½œå¹¶æ‰§è¡Œç¯å¢ƒ step
        5. æ›´æ–°æŒä¹…åŒ–çš„ç¯å¢ƒçŠ¶æ€

        Args:
            messages: æ¥è‡ª dataset çš„æ¶ˆæ¯ï¼ŒåŒ…å«ç¯å¢ƒæ ‡è¯†ä¿¡æ¯
            sampling_params: LLM é‡‡æ ·å‚æ•°
            trajectory: åŒ…å«global_stepsç­‰ä¿¡æ¯çš„è½¨è¿¹å­—å…¸
        """
        """
        Returns:
            AgentLoopOutput: å•æ­¥äº¤äº’æ•°æ®
        """
        # ä»trajectoryè·å–global_steps
        if trajectory and "step" in trajectory:
            self.current_global_steps = trajectory["step"]
        start_time = time.time()
        request_id = uuid4().hex

        # 1. ä» messages ä¸­æå–ç¯å¢ƒæ ‡è¯†ç¬¦å¹¶æ˜ å°„åˆ°æœ¬å®ä¾‹çš„ç¯å¢ƒ
        episode_id = 0
        if messages and len(messages) > 0:
            initial_message = messages[0]
            if isinstance(initial_message, dict):
                episode_id = initial_message.get("episode_id", 0)

        # å°†å…¨å±€episode_idæ˜ å°„åˆ°å½“å‰å®ä¾‹çš„ç¯å¢ƒä¸­çš„ä¸€ä¸ª
        local_env_index = (
            episode_id % self._envs_per_worker
        )  # åŠ¨æ€æ˜ å°„åˆ° 0, 1, ..., envs_per_worker-1
        env_id = f"env_{local_env_index}"

        # 2. è·å–å¯¹åº”çš„æŒä¹…åŒ–ç¯å¢ƒ
        env_data = self._get_environment(env_id)

        # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–° episodeï¼ˆå½“å‰è§‚å¯Ÿä¸º None è¡¨ç¤ºä¸Šä¸ª episode ç»“æŸäº†ï¼‰
        if env_data["current_obs"] is None:
            self._reset_environment(env_id)
            env_data = self._get_environment(env_id)  # é‡æ–°è·å–æ›´æ–°åçš„æ•°æ®

        # 3. è·å–å½“å‰ç¯å¢ƒè§‚å¯Ÿ
        wrapped_obs = env_data["env_wrapper"].wrap_observation(env_data["current_obs"])

        # 4. ç”Ÿæˆ LLM å“åº”
        prompt_list = [{"role": "user", "content": wrapped_obs}]
        formatted_text = self.tokenizer.apply_chat_template(
            prompt_list,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False,  # è®­ç»ƒæ•ˆç‡è€ƒè™‘ï¼Œç¦ç”¨æ€è€ƒæ¨¡å¼
        )

        prompt_ids = self.tokenizer.encode(formatted_text, add_special_tokens=True)

        # LLM ç”Ÿæˆå“åº”
        response_ids = await self.server_manager.generate(
            request_id=f"{request_id}_{env_data['episode_id']}_step{env_data['episode_step_count']}",
            prompt_ids=prompt_ids,
            sampling_params=sampling_params,
        )

        # è§£ç å“åº”
        response_text = self.tokenizer.decode(response_ids, skip_special_tokens=True)

        # 5. æ‰§è¡Œç¯å¢ƒäº¤äº’
        try:
            action_id = env_data["env_wrapper"].parse_llm_response(response_text)
        except Exception as e:
            print(f"âš ï¸ Action parsing failed: {e}, using default DO action")
            action_id = 5  # é»˜è®¤ DO åŠ¨ä½œ

        # æ‰§è¡ŒåŠ¨ä½œå¹¶æ›´æ–°æŒä¹…åŒ–çš„ç¯å¢ƒçŠ¶æ€
        new_obs, new_state, reward, done, _ = env_data["env_wrapper"].step(
            env_data["current_state"], action_id
        )

        # æ›´æ–°æŒä¹…åŒ–çš„ç¯å¢ƒçŠ¶æ€
        env_data["current_obs"] = new_obs
        env_data["current_state"] = new_state
        env_data["episode_step_count"] += 1

        # æ›´æ–°ç´¯ç§¯å¥–åŠ±
        episode_id = env_data["episode_id"]
        if episode_id not in self.__class__._episode_cumulative_rewards:
            self.__class__._episode_cumulative_rewards[episode_id] = 0.0
        self.__class__._episode_cumulative_rewards[episode_id] += reward

        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®ç¯å¢ƒ (ç¯å¢ƒè¿”å›done=True æˆ– è¶…è¿‡æœ€å¤§æ­¥æ•°é™åˆ¶)
        max_steps_reached = (
            env_data["episode_step_count"] >= self.__class__._max_episode_steps
        )
        # check global step
        if done or max_steps_reached:
            cumulative_reward = self.__class__._episode_cumulative_rewards[episode_id]
            end_reason = (
                "environment done"
                if done
                else f"max steps ({self.__class__._max_episode_steps}) reached"
            )
            print(
                f"ğŸ Episode {episode_id} finished after {env_data['episode_step_count']} steps ({end_reason}), cumulative reward: {cumulative_reward:.3f}"
            )

            # è®¡ç®—rewardå æœ€å¤§rewardçš„æ¯”ä¾‹
            reward_percentage = (
                cumulative_reward / self.__class__._max_craftax_reward * 100.0
            )

            # ç›´æ¥è®°å½•åˆ°wandb
            try:
                import wandb

                if wandb.run is not None:
                    # å°è¯•ä»Verlè®­ç»ƒæ¡†æ¶è·å–å½“å‰global_step
                    current_global_step = self._get_current_global_step()

                    # è®¡ç®—å…¨å±€ç¯å¢ƒäº¤äº’æ­¥æ•°: (update_step * rollout_n + rollout_step) * num_envs
                    # ç®€åŒ–ç‰ˆæœ¬ï¼šå‡è®¾rollout_step=0ï¼ˆæ‰¹æ¬¡å¼€å§‹ï¼‰
                    global_env_steps = (
                        current_global_step
                        * self.__class__._rollout_n
                        * self.__class__._num_envs
                    )

                    wandb.log(
                        {
                            "craftax/reward_percentage": reward_percentage,
                            "craftax/cumulative_reward": cumulative_reward,
                        },
                        step=global_env_steps,
                    )
                    print(
                        f"ğŸ“Š Wandb logged: Global step {current_global_step}, Env steps {global_env_steps}"
                    )
                    print(
                        f"    Reward: {cumulative_reward:.3f}, Percentage: {reward_percentage:.2f}%"
                    )
                else:
                    print(
                        f"ğŸ“Š No wandb run active, Episode {episode_id}, Reward: {cumulative_reward:.3f}, Percentage: {reward_percentage:.2f}%"
                    )
            except ImportError:
                print(
                    f"ğŸ“Š Wandb not available, Episode {episode_id}, Reward: {cumulative_reward:.3f}, Percentage: {reward_percentage:.2f}%"
                )

            # æ¸…ç†å·²å®Œæˆepisodeçš„å¥–åŠ±è®°å½•
            del self.__class__._episode_cumulative_rewards[episode_id]

            # ç¯å¢ƒç»“æŸï¼Œæ ‡è®°éœ€è¦é‡ç½®ï¼ˆä½†ä¸é”€æ¯ç¯å¢ƒï¼‰
            env_data["current_obs"] = None
            env_data["current_state"] = None

        # 5. åœ¨ response åé¢åŠ ä¸Šå½“å‰æ­¥éª¤çš„å¥–åŠ±ä¿¡æ¯
        reward_info = f" [Reward: {reward:.3f}]"
        reward_tokens = self.tokenizer.encode(reward_info, add_special_tokens=False)
        response_ids.extend(reward_tokens)

        # åˆ›å»ºå“åº”æ©ç 
        response_mask = [1] * len(response_ids)

        # 6. æ„å»ºæœ€ç»ˆè¾“å‡º
        # ä½¿ç”¨é…ç½®ä¸­çš„åºåˆ—é•¿åº¦é™åˆ¶
        final_prompt_ids = prompt_ids[: self.prompt_length]
        final_response_ids = response_ids[: self.response_length]
        final_response_mask = response_mask[: len(final_response_ids)]

        # æ„å»ºæŒ‡æ ‡
        total_time = time.time() - start_time
        metrics = {
            "generate_sequences": total_time * 0.8,  # å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç”Ÿæˆ
            "tool_calls": total_time * 0.2,  # å°‘éƒ¨åˆ†æ—¶é—´åœ¨ç¯å¢ƒäº¤äº’
            "step_reward": reward,  # å½“å‰æ­¥å¥–åŠ±
            "episode_step": env_data["episode_step_count"],  # å½“å‰ episode æ­¥æ•°
            "done": done,  # æ˜¯å¦ç»“æŸ
            "action_id": action_id,  # æ‰§è¡Œçš„åŠ¨ä½œ
            "episode_id": str(env_data["episode_id"]),  # episode æ ‡è¯†
            "env_id": env_id,  # ç¯å¢ƒæ ‡è¯†
        }

        # å¦‚æœepisodeç»“æŸï¼Œæ·»åŠ wandbè®°å½•æ•°æ®
        if done:
            cumulative_reward = self.__class__._episode_cumulative_rewards.get(
                episode_id, 0.0
            )
            reward_percentage = (
                cumulative_reward / self.__class__._max_craftax_reward * 100.0
            )
            metrics.update(
                {
                    "episode_finished": True,
                    "cumulative_reward": cumulative_reward,
                    "reward_percentage": reward_percentage,  # è¿™æ˜¯æˆ‘ä»¬è¦ç»˜åˆ¶çš„çºµåæ ‡
                }
            )

        return AgentLoopOutput(
            prompt_ids=final_prompt_ids,
            response_ids=final_response_ids,
            response_mask=final_response_mask,
            num_turns=1,  # æ¯æ¬¡åªæ‰§è¡Œä¸€æ­¥
            metrics=AgentLoopMetrics(**metrics),
        )
